apiVersion: v1
kind: Service
metadata:
  name: headless-svc
  namespace: default
spec:
  clusterIP: None
  selector:
    job-name: ptxla-hello-world
---
apiVersion: batch/v1
kind: Job
metadata:
  name: ptxla-hello-world
  namespace: default
spec:
  backoffLimit: 1
  completionMode: Indexed
  completions: 2 # xw32 todo: change to 4
  parallelism: 2 # xw32 todo: change to 4
  template:
    spec:
      nodeSelector:
        cloud.google.com/gke-accelerator: nvidia-tesla-v100
      restartPolicy: Never
      subdomain: headless-svc
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
      containers:
      - name: ptxla-worker
        image: us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:nightly_3.8_cuda_12.1
        # image: gcr.io/tpu-pytorch/myptxla/hello:latest
        command: ["/bin/bash", "-c", "--"]
        # args: ["sleep 3600"]
        args: ["export PATH=/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin; export LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64; while true; do ip=$(getent hosts ptxla-hello-world-0.headless-svc | awk {'print $1'}); if [ $? -eq 0 ] && [ \"${ip}\" != \"\" ]; then break; else sleep 10; fi; done; echo $ip; git clone https://github.com/pytorch/xla.git; PJRT_DEVICE=CUDA torchrun --nnodes=2 --node_rank=$JOB_COMPLETION_INDEX --nproc_per_node=2 --rdzv_endpoint=$ip:12355 xla/test/test_train_mp_imagenet.py  --fake_data --pjrt_distributed --batch_size=128 --num_epochs=1"]
        # args: ["cd /data/tensorflow-mnist-example; pip install -r requirements.txt; python tensorflow_mnist_train_distributed.py"]
        ports:
        - containerPort: 1234
        resources:
          limits:
            nvidia.com/gpu: 2
